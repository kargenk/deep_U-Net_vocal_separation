# -*- coding: utf-8 -*-
"""network.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15x3CCf-P1b-LeyeubWv0IfawRrvI76hN
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-

from chainer import Chain, serializers, optimizers, config
import chainer.functions as F
import chainer.links as L
import numpy as np
import cupy as cp
import const as C

class UNet(Chain):
    """
    Network Architecture of UNet, Core Class
    """
    def __init__(self):
        super().__init__()
        
        with self.init_scope():
            self.conv1 = L.Convolution2D(1, 16, 4, 2, 1)
            self.norm1 = L.BatchNormalization(16)
            self.conv2 = L.Convolution2D(16, 32, 4, 2, 1)
            self.norm2 = L.BatchNormalization(32)
            self.conv3 = L.Convolution2D(32, 64, 4, 2, 1)
            self.norm3 = L.BatchNormalization(64)
            self.conv4 = L.Convolution2D(64, 128, 4, 2, 1)
            self.norm4 = L.BatchNormalization(128)
            self.conv5 = L.Convolution2D(128, 256, 4, 2, 1)
            self.norm5 = L.BatchNormalization(256)
            self.conv6 = L.Convolution2D(256, 512, 4, 2, 1)
            self.norm6 = L.BatchNormalization(512)
            self.deconv1 = L.Deconvolution2D(512, 256, 4, 2, 1)
            self.denorm1 = L.BatchNormalization(256)
            self.deconv2 = L.Deconvolution2D(512, 128, 4, 2, 1)
            self.denorm2 = L.BatchNormalization(128)
            self.deconv3 = L.Deconvolution2D(256, 64, 4, 2, 1)
            self.denorm3 = L.BatchNormalization(64)
            self.deconv4 = L.Deconvolution2D(128, 32, 4, 2, 1)
            self.denorm4 = L.BatchNormalization(32)
            self.deconv5 = L.Deconvolution2D(64, 16, 4, 2, 1)
            self.denorm5 = L.BatchNormalization(16)
            self.deconv6 = L.Deconvolution2D(32, 1, 4, 2, 1)
    
    def __call__(self, X):
        h1 = F.leaky_relu(self.norm1(self.conv1(X)))
        h2 = F.leaky_relu(self.norm2(self.conv2(h1)))
        h3 = F.leaky_relu(self.norm3(self.conv3(h2)))
        h4 = F.leaky_relu(self.norm4(self.conv4(h3)))
        h5 = F.leaky_relu(self.norm5(self.conv5(h4)))
        h6 = F.leaky_relu(self.norm6(self.conv6(h5)))
        dh = F.relu(F.dropout(self.denorm1(self.deconv1(h6))))
        dh = F.relu(F.dropout(self.denorm2(self.deconv2(F.concat((dh, h5))))))
        dh = F.relu(F.dropout(self.denorm3(self.deconv3(F.concat((dh, h4))))))
        dh = F.relu(self.denorm4(self.deconv4(F.concat((dh, h3)))))
        dh = F.relu(self.denorm5(self.deconv5(F.concat((dh, h2)))))
        dh = F.sigmoid(self.deconv6(F.concat((dh, h1))))
        return dh
    
    def load(self, file_name='unet.model'):
        serializers.load_npz(file_name, self)
    
    def save(self, file_name='unet.model'):
        serializers.save_npz(file_name, self)
        
class Trainer_UNet(Chain):
    """
    cantain loss function for train Class
    """
    def __init__(self, unet):
        super().__init__()
        
        with self.init_scope():
            self.unet = unet
    
    def __call__(self, X, y):
        O = self.unet(X)  # UNetを通した後のマスク
        self.loss = F.mean_absolute_error(X * O, y)
        return self.loss

def train(X_list, y_list, epoch=40, save_file='unet.model'):
    assert(len(X_list) == len(y_list))
    
    # モデルのインスタンス化
    unet = UNet()
    model = Trainer_UNet(unet)
    optimizer = optimizers.Adam().setup(model)  # オプティマイザのセッティング
    
    # 各種セッティングの有効化
    model.to_gpu(0)
    config.train = True
    config.enable_backprop = True
    
    music_count = len(X_list)
    music_length = [x.shape[1] for x in X_list]
    sub_epoch = sum(music_length) // C.PATCH_LENGTH // C.BATCH_SIZE * 4
    
    for ep in range(epoch):
        sum_loss = 0.0
        for sub_ep in range(sub_epoch):
            X = np.zeros((C.BATCH_SIZE, 1, 512, C.PATCH_LENGTH),
                         dtype='float32')
            y = np.zeros((C.BATCH_SIZE, 1, 512, C.PATCH_LENGTH),
                         dtype='float32')
            music_index = np.random.randint(0, music_count, C.BATCH_SIZE)
            
            for i in range(C.BATCH_SIZE):
                rand_index = np.random.randint(
                    music_length[music_index[i]] - C.PATCH_LENGTH - 1)
                X[i, 0, :, :] = \
                    X_list[music_index[i]][1:, rand_index:rand_index + C.PATCH_LENGTH]
                y[i, 0, :, :] = \
                    y_list[music_index[i]][1:, rand_index:rand_index + C.PATCH_LENGTH]
            
            optimizer.update(model, cp.asarray(X), cp.asarray(y))
            sum_loss += model.loss.data * C.BATCH_SIZE
        
        print('epoch: %d/%d  loss=%.3f' % (ep+1, epoch, sum_loss))
    unet.save(save_file)

